{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qyNfWI2--pl"
      },
      "outputs": [],
      "source": [
        "%pip install qdrant_client llama-index llama-index-vector-stores-qdrant llama-index-llms-openai llama-index-agent-openai\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_OWnvIYCa2-"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNpw4ewT-fSz",
        "outputId": "580fa7e0-ce1a-4bd9-b144-ea887c83cfc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:llama_index.vector_stores.qdrant.base:Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from llama_index.core.vector_stores.types import VectorStore\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
        "\n",
        "\n",
        "def get_vector_store() -> VectorStore:\n",
        "    COLLECTION_NAME = \"document\"\n",
        "\n",
        "    aclient = AsyncQdrantClient(\n",
        "        url=\"https://6a4f58ad-84bd-4868-b9ec-48fdb5c59ef3.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "        api_key=os.getenv('QDRANT_API_KEY'),\n",
        "    )\n",
        "    client = QdrantClient(\n",
        "        url=\"https://6a4f58ad-84bd-4868-b9ec-48fdb5c59ef3.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "        api_key=os.getenv('QDRANT_API_KEY'),\n",
        "    )\n",
        "\n",
        "    return QdrantVectorStore(\n",
        "        client=client,\n",
        "        aclient=aclient,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "    )\n",
        "\n",
        "qdrant_store = get_vector_store()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8C0r9vxDCff",
        "outputId": "726f5c52-0a7b-4c70-eadf-de1ac0d6e6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing collection 'document' deleted.\n",
            "New collection 'document' created.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<qdrant_client.qdrant_client.QdrantClient at 0x7c7633bc31d0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "\n",
        "def init_qdrant():\n",
        "    \"\"\"\n",
        "    Initialize Qdrant client, remove the existing 'document' collection (if it exists),\n",
        "    and create a new 'document' collection.\n",
        "    \"\"\"\n",
        "    client = QdrantClient(\n",
        "        url=\"https://6a4f58ad-84bd-4868-b9ec-48fdb5c59ef3.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "        api_key=userdata.get('QDRANT_API_KEY'),\n",
        "    )\n",
        "\n",
        "    # Check if the collection exists and delete it if it does\n",
        "    try:\n",
        "        client.delete_collection(collection_name=\"document\")\n",
        "        print(\"Existing collection 'document' deleted.\")\n",
        "    except UnexpectedResponse as e:\n",
        "        if \"404\" not in str(e):\n",
        "            # If the error is not a 404 (collection not found), re-raise the exception\n",
        "            raise e\n",
        "        # If the collection doesn't exist, no action is needed\n",
        "        print(\"Collection 'document' does not exist. Proceeding to create it.\")\n",
        "\n",
        "    # Create a new 'document' collection\n",
        "    client.create_collection(\n",
        "        collection_name=\"document\",\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=1536, distance=models.Distance.COSINE\n",
        "        ),\n",
        "    )\n",
        "    print(\"New collection 'document' created.\")\n",
        "\n",
        "    return client\n",
        "\n",
        "init_qdrant()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t6x64cPJ_rpV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "from llama_index.core.agent import AgentRunner\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.settings import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.tools import BaseTool, QueryEngineTool\n",
        "from llama_index.core.chat_engine.types import ChatMessage\n",
        "\n",
        "# Initialize basic components\n",
        "def get_chat_engine(vector_store=None):\n",
        "    SYSTEM_PROMPT = os.getenv(\"SYSTEM_PROMPT\", \"You are a helpful assistant.\")\n",
        "    tools: List[BaseTool] = []\n",
        "\n",
        "    # Create index from vector store if provided\n",
        "    if vector_store is not None:\n",
        "        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "        query_engine = index.as_query_engine()\n",
        "        query_tool = QueryEngineTool.from_defaults(query_engine=query_engine)\n",
        "        tools.append(query_tool)\n",
        "\n",
        "    return AgentRunner.from_llm(\n",
        "        llm=Settings.llm,\n",
        "        tools=tools,\n",
        "        system_prompt=SYSTEM_PROMPT,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "# Function to simulate a chat interaction\n",
        "async def chat_interaction(message: str, chat_history: List[ChatMessage] = None):\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    chat_engine = get_chat_engine(qdrant_store)\n",
        "    chat_response = await chat_engine.achat(message, chat_history)\n",
        "\n",
        "    return chat_response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd1V627WAUZ1",
        "outputId": "4b867798-5a81-4fbf-997f-54e4c05a3861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Hello, how are you?\n",
            "Response: Hello! I'm here and ready to assist you. How can I help you today?\n",
            "Added user message to memory: Tell me more about machine learning\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Machine learning\"}\n",
            "Got output: Empty Response\n",
            "========================\n",
            "\n",
            "Response with history: Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn from and make predictions or decisions based on data. It involves the use of statistical techniques to enable machines to improve their performance on a specific task without being explicitly programmed. Machine learning is widely used in various applications such as image recognition, natural language processing, and recommendation systems.\n"
          ]
        }
      ],
      "source": [
        "async def main():\n",
        "    # Test single message\n",
        "    response = await chat_interaction(\"Hello, how are you?\")\n",
        "    print(\"Response:\", response)\n",
        "\n",
        "    # Test with chat history\n",
        "    history = [\n",
        "        ChatMessage(role=\"user\", content=\"What is AI?\"),\n",
        "        ChatMessage(role=\"assistant\", content=\"AI is artificial intelligence...\")\n",
        "    ]\n",
        "    response = await chat_interaction(\"Tell me more about machine learning\", history)\n",
        "    print(\"Response with history:\", response)\n",
        "\n",
        "# Run the async function\n",
        "import asyncio\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LYx8ydx1C2iX"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "Settings.embed_model = OpenAIEmbedding(\n",
        "    model=\"text-embedding-3-small\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sPmdl_EZFg0v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from llama_index.core.evaluation import (\n",
        "    QueryResponseEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    FaithfulnessEvaluator,\n",
        "    ContextRelevancyEvaluator\n",
        ")\n",
        "from llama_index.core.chat_engine.types import ChatMessage\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from llama_index.core.schema import Document\n",
        "\n",
        "class MedicalChatbotEvaluator:\n",
        "    def __init__(self, chat_engine, vector_store=None):\n",
        "        self.chat_engine = chat_engine\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "        # Initialize evaluators\n",
        "        self.response_evaluator = QueryResponseEvaluator()\n",
        "        self.relevancy_evaluator = RelevancyEvaluator()\n",
        "        self.faithfulness_evaluator = FaithfulnessEvaluator()\n",
        "        self.context_relevancy_evaluator = ContextRelevancyEvaluator()\n",
        "\n",
        "    async def evaluate_single_response(self,\n",
        "                                     question: str,\n",
        "                                     ground_truth: str,\n",
        "                                     chat_history: List[ChatMessage] = None) -> Dict:\n",
        "        \"\"\"Evaluate a single question-answer pair\"\"\"\n",
        "        try:\n",
        "            # Get response from chatbot\n",
        "            response = await self.chat_engine.achat(question, chat_history)\n",
        "\n",
        "            # Extract contexts\n",
        "            contexts = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                contexts = [str(ctx.node.text) for ctx in response.source_nodes]\n",
        "            elif self.vector_store:\n",
        "                # If no source nodes but vector store exists, get relevant documents\n",
        "                results = self.vector_store.similarity_search(question, k=3)\n",
        "                contexts = [str(doc.text) for doc in results]\n",
        "\n",
        "            # Ensure we have at least one context\n",
        "            if not contexts:\n",
        "                contexts = [response.response]  # Use response as context if no other context available\n",
        "\n",
        "            # Run evaluations\n",
        "            response_quality = await self.response_evaluator.aevaluate(\n",
        "                query=question,\n",
        "                response=response.response,\n",
        "                ground_truth=ground_truth\n",
        "            )\n",
        "\n",
        "            relevancy_score = await self.relevancy_evaluator.aevaluate(\n",
        "                query=question,\n",
        "                response=response.response,\n",
        "                contexts=contexts\n",
        "            )\n",
        "\n",
        "            faithfulness_score = await self.faithfulness_evaluator.aevaluate(\n",
        "                query=question,\n",
        "                response=response.response,\n",
        "                contexts=contexts\n",
        "            )\n",
        "\n",
        "            context_relevancy = await self.context_relevancy_evaluator.aevaluate(\n",
        "                query=question,\n",
        "                contexts=contexts\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'question': question,\n",
        "                'response': response.response,\n",
        "                'ground_truth': ground_truth,\n",
        "                'response_quality': response_quality.score,\n",
        "                'relevancy': relevancy_score.score,\n",
        "                'faithfulness': faithfulness_score.score,\n",
        "                'context_relevancy': context_relevancy.score,\n",
        "                'sources_used': len(contexts)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {question}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {\n",
        "                'question': question,\n",
        "                'response': None,\n",
        "                'ground_truth': ground_truth,\n",
        "                'response_quality': 0.0,\n",
        "                'relevancy': 0.0,\n",
        "                'faithfulness': 0.0,\n",
        "                'context_relevancy': 0.0,\n",
        "                'sources_used': 0\n",
        "            }\n",
        "\n",
        "    async def evaluate_dataset(self, dataset_name: str = \"bigbio/med_qa\", split: str = \"test\"):\n",
        "        \"\"\"Evaluate using a HuggingFace dataset\"\"\"\n",
        "        try:\n",
        "            # Load dataset\n",
        "            dataset = load_dataset(dataset_name, 'med_qa_en_source', split=split, trust_remote_code=True)\n",
        "            results = []\n",
        "\n",
        "            for item in tqdm(dataset):\n",
        "                try:\n",
        "                    # Adapt this according to your dataset structure\n",
        "                    question = item['question']\n",
        "                    ground_truth = item['answer']\n",
        "\n",
        "                    result = await self.evaluate_single_response(question, ground_truth)\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing item: {item}\")\n",
        "                    print(f\"Error: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            return pd.DataFrame([r for r in results if r is not None])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def calculate_metrics(self, results_df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Calculate aggregate metrics\"\"\"\n",
        "        if results_df.empty:\n",
        "            return {\n",
        "                'avg_response_quality': 0.0,\n",
        "                'avg_relevancy': 0.0,\n",
        "                'avg_faithfulness': 0.0,\n",
        "                'avg_context_relevancy': 0.0,\n",
        "                'avg_sources_used': 0.0,\n",
        "                'std_response_quality': 0.0,\n",
        "                'std_relevancy': 0.0,\n",
        "                'std_faithfulness': 0.0,\n",
        "                'std_context_relevancy': 0.0\n",
        "            }\n",
        "\n",
        "        metrics = {\n",
        "            'avg_response_quality': results_df['response_quality'].mean(),\n",
        "            'avg_relevancy': results_df['relevancy'].mean(),\n",
        "            'avg_faithfulness': results_df['faithfulness'].mean(),\n",
        "            'avg_context_relevancy': results_df['context_relevancy'].mean(),\n",
        "            'avg_sources_used': results_df['sources_used'].mean(),\n",
        "            'std_response_quality': results_df['response_quality'].std(),\n",
        "            'std_relevancy': results_df['relevancy'].std(),\n",
        "            'std_faithfulness': results_df['faithfulness'].std(),\n",
        "            'std_context_relevancy': results_df['context_relevancy'].std()\n",
        "        }\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zv2GLV_6GHgz",
        "outputId": "55a11739-d4ca-4ec1-91c2-1f32952fb70e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "async def main():\n",
        "    chat_engine = get_chat_engine(qdrant_store)\n",
        "    evaluator = MedicalChatbotEvaluator(chat_engine, qdrant_store)\n",
        "    results_df = await evaluator.evaluate_dataset()\n",
        "\n",
        "    metrics = evaluator.calculate_metrics(results_df)\n",
        "\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "    results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    metrics_to_plot = ['response_quality', 'relevancy', 'faithfulness', 'context_relevancy']\n",
        "    plt.boxplot([results_df[metric] for metric in metrics_to_plot], labels=metrics_to_plot)\n",
        "    plt.title(\"Distribution of Evaluation Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.savefig(\"evaluation_metrics.png\")\n",
        "    plt.close()\n",
        "\n",
        "import asyncio\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIRF_kA4GcWZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
